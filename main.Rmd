---
title: "Linear Regression Final Project"
author: "Noah Love, Frederick Cordova, Giovanni Sanchez, Matthew Chen"
date: "4/8/2021"
output: html_document
---


```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(glmnet)
library(caret)
library(elasticnet)
library(reshape2)
library(plotly)
library(ggcorrplot)
```

```{r message=FALSE, warning=FALSE}
df <- as_tibble(read.table("winequality-red.csv", sep=";", header = TRUE))
df_white <- as_tibble(read.table("winequality-white.csv", sep=";", header = TRUE))

index = sample(1:nrow(df), 0.7*nrow(df)) 

train = df[index,] # Create the training data 
test = df[-index,] # Create the test data

dim(train)
dim(test)


```

## Rudimentary LM on whole dataset

```{r}
lm_basic <- lm(data = df, quality ~ .)

summary(lm_basic)
```
Volatile acidity, chlorides, total sulfur dioxide, sulphates and alcohol are all very statistically significant, although our adjusted $R^2$ is still quite low. 


## Ridge regression

```{r}
train_control <- trainControl(method  = "cv", number = 5)

model_ridge <- train(quality ~ .,
                     data = df,
                     method = "ridge",           # method
                     trControl = train_control)        # cross validation

model_ridge

#something doesn't seem to be working here


```

```{r}
model_stepwise <- train(quality ~ .,
                        data = df,
                        method = "glmStepAIC",
                        trControl = train_control)
#model_stepwise
```


## Explore anything highly correlated:

```{r}
correlation_df <- cor(df)

#correlation_df

# fixed acidity to ph and sulphates
```

```{r}
correlation_df_melt <- melt(correlation_df)

gz <- ggplot(correlation_df_melt, mapping = aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  theme(text = element_text(size = 8)) + 
  ggtitle("Heat map for correlation") + 
  ylab("")+
  xlab("")+
  scale_fill_distiller(palette = "RdPu")

ggplotly(gz, tooltip = "text")

```

```{r}
minimized_lm_df <- df %>% 
  select(-free.sulfur.dioxide, -citric.acid, -density, -volatile.acidity, -pH)

minimized_lm <- lm(data = minimized_lm_df, quality ~ .)

summary(minimized_lm)

```


## correlations
```{r}
findCorrelation(
  cor(df),
  cutoff = 0.5,
  verbose = TRUE,
  names = TRUE
)
```

```{r}
ggcorrplot(correlation_df, hc.order = TRUE, type = "lower")
```


```{r}
df_less_acid <- df %>% 
  select(-citric.acid, -fixed.acidity)

lm_less_acid <- lm(data = df_less_acid, quality ~ .)
summary(lm_less_acid)
```
## Lasso

```{r}
plot(df)

```

## Full scaled analysis


```{r}
colnames(df)

cols = c('fixed.acidity', 'volatile.acidity', 'citric.acid', 'residual.sugar', 'chlorides', 'free.sulfur.dioxide', 'total.sulfur.dioxide', 'density', 'pH', 'sulphates', 'alcohol')

pre_proc_val <- preProcess(train[,cols], method = c("center", "scale"))

train[,cols] = predict(pre_proc_val, train[,cols])
test[,cols] = predict(pre_proc_val, test[,cols])

summary(train)

```
#generic LM
```{r}
lm_2 <- lm(quality ~ ., data = train)
summary(lm_2)
```
# Regularization

```{r}
cols_reg = c('fixed.acidity', 'volatile.acidity', 'citric.acid', 'residual.sugar', 'chlorides', 'free.sulfur.dioxide', 'total.sulfur.dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality')

dummies <- dummyVars(quality ~ ., data = df[,cols_reg])

train_dummies = predict(dummies, newdata = train[,cols_reg])

test_dummies = predict(dummies, newdata = test[,cols_reg])

print(dim(train_dummies)); print(dim(test_dummies))
```

## Ridge Regression 

```{r}
x = as.matrix(train_dummies)
y_train = train$quality

x_test = as.matrix(test_dummies)
y_test = test$quality

lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)

summary(ridge_reg)
```

### Optimal Lambda
```{r}
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
```


### Formal metrics for Ridge

```{r}
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  
  # Model performance metrics
data.frame(
  RMSE = RMSE,
  Rsquare = R_square
)
  
}

# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
eval_results(y_train, predictions_train, train)

# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
eval_results(y_test, predictions_test, test)

```


## Lasso
```{r}
lambdas <- 10^seq(2, -3, by = -.1)

# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

# Best 
lambda_best <- lasso_reg$lambda.min 
lambda_best
```


```{r}
lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_train <- predict(lasso_model, s = lambda_best, newx = x)
eval_results(y_train, predictions_train, train)

predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test)
eval_results(y_test, predictions_test, test)
```


## Mix of ridge and lasso (elasticnet)

```{r}
# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_reg <- train(quality ~ .,
                           data = train,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune
```


## Metrics for elasticnet

```{r}
# Make predictions on training set
predictions_train <- predict(elastic_reg, x)
eval_results(y_train, predictions_train, train) 

# Make predictions on test set
predictions_test <- predict(elastic_reg, x_test)
eval_results(y_test, predictions_test, test)

```


